Iniciar servidor com:
python -m llama_cpp.server --host 0.0.0.0 --model .\model\Llama-3-8B-Instruct.Q6_K.gguf --n_ctx 2048 --n_gpu_layers 24

Baixar modelo diretamente desse link: https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF e ajustar c√≥digo para o nome do arquivo
